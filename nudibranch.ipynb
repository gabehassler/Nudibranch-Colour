{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for summarizing Nudibranch phylogenetic factor analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data pre-processing\n",
    "## 1.1 - Import data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, CSV, Statistics, BEASTDataPrep, BeastUtils.MatrixUtils, Clustering\n",
    "import Random\n",
    "Random.seed!(666) # number of the BEAST\n",
    "\n",
    "cd(@__DIR__)\n",
    "\n",
    "raw_data_path = \"Nelson Bay 2cm raw_manually curated_26.7.22.csv\"\n",
    "\n",
    "raw_data = CSV.read(raw_data_path, DataFrame, missingstring=\"NA\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace species names.\n",
    "There are some mistmatches between the original data file and the final phylogeny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "    \"Theracera pennigera\" => \"Thecacera pennigera\",\n",
    "    \"Chromodoris cf striatella QLD\" => \"Chromodoris cf striatella\",\n",
    "    \"Tenelia sibogae\" => \"Tenellia sibogae\",\n",
    "    \"Dendrodoris gunnamatta\" => \"Dendrodoris krusensterni\",\n",
    "    \"Tayuva lilacina\" => \"Discodoris sp\",\n",
    "    \"Mariona sp\" => \"Marionia sp\",\n",
    "    \"Pleurobranchus peronii\" => \"Pleurobranchus peroni\"\n",
    "]\n",
    "\n",
    "for r in replacements\n",
    "    replace!(raw_data.Species, r)\n",
    "end\n",
    "\n",
    "CSV.write(\"corrected.csv\", raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Subset to QCPA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = eltype.(eachcol(raw_data))\n",
    "\n",
    "\n",
    "numeric_cols = findall(x -> x <: Union{Float64, Missing}, col_types)\n",
    "\n",
    "\n",
    "n_taxa, n_cols = size(raw_data)\n",
    "\n",
    "nonnumeric_cols = setdiff(1:n_cols, numeric_cols)\n",
    "names(raw_data)[nonnumeric_cols] # non-numeric values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Data Transformations\n",
    "We need to figure out appropriate transformations of the data to \"normalize\" it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_numeric = length(numeric_cols)\n",
    "numeric_names = names(raw_data)[numeric_cols]\n",
    "\n",
    "metadata = DataFrame(trait = numeric_names, mean = zeros(n_numeric), sd = zeros(n_numeric),\n",
    "    maximum = zeros(n_numeric), minimum = zeros(n_numeric),\n",
    "    perc_missing = zeros(n_numeric))\n",
    "\n",
    "\n",
    "for i = 1:n_numeric\n",
    "    col = numeric_cols[i]\n",
    "    col_data = raw_data[:, col]\n",
    "    present_inds = (!ismissing).(col_data)\n",
    "    complete_data = col_data[present_inds]\n",
    "    metadata.mean[i] = mean(complete_data)\n",
    "    metadata.sd[i] = std(complete_data)\n",
    "    metadata.maximum[i] = maximum(complete_data)\n",
    "    metadata.minimum[i] = minimum(complete_data)\n",
    "    metadata.perc_missing[i] = 1.0 - sum(present_inds) / n_taxa\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_svg = \"transform_check.svg\"\n",
    "\n",
    "\n",
    "pvals = plot_transformed_data(raw_data[!, numeric_cols], overwrite=true, svg_path = transform_svg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(transform_svg) do f\n",
    "   display(\"image/svg+xml\", read(f, String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function find_cat(x::String)\n",
    "    return split(x, '.')[1]\n",
    "end\n",
    "\n",
    "categories = unique(find_cat.(pvals.trait))\n",
    "n_cats = length(categories)\n",
    "cat_dfs = Vector{DataFrame}(undef, n_cats)\n",
    "for i = 1:n_cats\n",
    "    cat = categories[i]\n",
    "    rows = findall(x -> startswith(x, cat), pvals.trait)\n",
    "    cat_dfs[i] = pvals[rows, :]\n",
    "    display(cat_dfs[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above, it looks like `Col` and `Lum` should be log-transformed, `GabRat` should be logit-transformed, and `BSA` should be un-transformed.\n",
    "\n",
    "CAA and VCA are ambiguous. As CAA has a high number of true `0` values that would need to be adjusted for log-transformation, we do not transform CAA. Some of the log-transformed VCA quantities seem to fit normal data well, so we opt to log-transform them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_values = [\"Col\", \"Lum\", \"VCA\"]\n",
    "logit_values = [\"GabRat\"]\n",
    "other_values = [\"BSA\", \"CAA\"]\n",
    "\n",
    "function logit(x::Float64)\n",
    "    return log(x / (1 - x))\n",
    "end\n",
    "\n",
    "function logit(x::Missing)\n",
    "    return missing\n",
    "end\n",
    "\n",
    "transformed_data = DataFrame()\n",
    "col_names = names(raw_data)\n",
    "\n",
    "for i = 1:size(raw_data, 2)\n",
    "    col = raw_data[:, i]\n",
    "    col_name = col_names[i]\n",
    "    col_start = split(col_name, '.')[1]\n",
    "    if eltype(col) <: Real\n",
    "        if col_start in log_values\n",
    "            transformed_col = log.(col)\n",
    "        elseif col_start in logit_values\n",
    "            transformed_col = logit.(col)\n",
    "        elseif col_start in other_values\n",
    "            transformed_col = copy(col)\n",
    "        else\n",
    "            error(\"unknown transformation\")\n",
    "        end\n",
    "    else\n",
    "        transformed_col = copy(col)\n",
    "    end\n",
    "    \n",
    "    transformed_data[!, col_name] = transformed_col\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Colinearity\n",
    "\n",
    "Many QCPA quanties are highly correlated and can be removed from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function check_colinear(df::DataFrame; threshold::Float64 = 0.9)\n",
    "    types = eltype.(eachcol(df))\n",
    "    @show length(types)\n",
    "    numeric = findall(x -> x <: Union{Real, Missing}, types)\n",
    "    sub_df = df[:, numeric]\n",
    "    \n",
    "    cols = names(sub_df)\n",
    "    n, p = size(sub_df)\n",
    "    @show n, p\n",
    "    C = MatrixUtils.missing_cor(sub_df)\n",
    "    \n",
    "    #find colinearity structure\n",
    "    dists = ones(p, p) - abs.(C)\n",
    "    clust = hclust(dists, linkage=:complete)\n",
    "    assignments = cutree(clust, h = 1.0 - threshold)\n",
    "\n",
    "    n_clusters = maximum(assignments)\n",
    "    clusters = [Int[] for i = 1:n_clusters]\n",
    "    for i = 1:p\n",
    "        push!(clusters[assignments[i]], i)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cor_df = DataFrame(trait = cols)\n",
    "    for i = 1:p\n",
    "        cor_df[!, cols[i]] = C[:, i]\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    return cor_df, clusters\n",
    "end\n",
    "\n",
    "trans_cor_df, trans_clusters = check_colinear(transformed_data)\n",
    "trans_clusters\n",
    "traits = trans_cor_df.trait\n",
    "\n",
    "ind = 0\n",
    "for clust in trans_clusters\n",
    "    ind += 1\n",
    "    println(\"cluster $ind: $(traits[clust])\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is high co-linearity between most traits that are split by vertical and horizontal patterns. For now, I'm going to just use the base value for those traits. If there are two traits in different categories that are highly correlated, I take both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_representatives = Dict{Int, Union{String,Vector{String}}}(\n",
    "    1 => \"Col.mean\",\n",
    "    2 => \"Col.sd\",\n",
    "    3 => \"Col.CoV\",\n",
    "    4 => [\"Col.skew\", \"Col.kurtosis\"],\n",
    "    5 => \"Lum.mean\",\n",
    "    6 => \"Lum.sd\",\n",
    "    7 => \"Lum.CoV\",\n",
    "    8 => \"Lum.skew\",\n",
    "    9 => \"Lum.kurtosis\",\n",
    "    10 => \"GabRat\",\n",
    "    11 => [\"CAA.Sc\", \"CAA.Hc\"],\n",
    "    12 => \"CAA.Jc\",\n",
    "    13 => \"CAA.St\",\n",
    "    14 => [\"CAA.Jt\", \"CAA.Scpl\"],\n",
    "    15 => \"CAA.Qc\",\n",
    "    16 => \"CAA.Ht\",\n",
    "    17 => \"CAA.Qt\",\n",
    "    18 => \"CAA.Qcpl\",\n",
    "    18 => \"CAA.C\",\n",
    "    20 => \"CAA.Qc.Hrz\",\n",
    "    21 => \"CAA.Qt.Hrz\",\n",
    "    22 => \"CAA.PT\",\n",
    "    23 => \"CAA.Asp\",\n",
    "    24 => \"VCA.ML\",\n",
    "    25 => \"VCA.sL\",\n",
    "    26 => \"VCA.CVL\",\n",
    "    27 => [\"VCA.MDmax\", \"VCA.MSsat\"],\n",
    "    28 => [\"VCA.sDmax\", \"VCA.sSsat\"],\n",
    "    29 => [\"VCA.CVDmax\", \"VCA.CVSsat\"],\n",
    "    30 => \"VCA.MSL\",\n",
    "    31 => \"VCA.sSL\",\n",
    "    32 => \"VCA.CVSL\",\n",
    "    33 => [\"VCA.MS\", \"VCA.sS\"],\n",
    "    34 => \"VCA.CVS\",\n",
    "    35 => [\"BSA.BML\", \"BSA.BMSL\"],\n",
    "    36 => [\"BSA.BsL\", \"BSA.BsSL\"],\n",
    "    37 => [\"BSA.BCVL\", \"BSA.BCVSL\"],\n",
    "    38 => [\"BSA.BMDmax\", \"BSA.BMSsat\"],\n",
    "    39 => [\"BSA.BsDmax\", \"BSA.BsSsat\"],\n",
    "    40 => \"BSA.BCVDmax\",\n",
    "    41 => \"BSA.BCVSsat\",\n",
    "    42 => \"BSA.BMS\",\n",
    "    43 => \"BSA.BsS\",\n",
    "    44 => \"BSA.BCVS\"\n",
    ")\n",
    "\n",
    "keep_traits = String[]\n",
    "for v in values(cluster_representatives)\n",
    "    keep_traits = [keep_traits; v]\n",
    "end\n",
    "\n",
    "for trait in keep_traits\n",
    "    @assert trait in traits\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - Create a new data frame to be used in PFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_data = [DataFrame(taxon = raw_data.Individual) transformed_data[!, keep_traits]]\n",
    "full_data = [DataFrame(taxon = raw_data.Individual) transformed_data[!, numeric_cols]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conforming the tree and trait data\n",
    "\n",
    "The first step is to replace the species names with individual names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_newick_path = \"final_tree_13_3_23.tre\"\n",
    "newick = read(original_newick_path, String)\n",
    "\n",
    "\n",
    "all_species = unique(raw_data.Species)\n",
    "\n",
    "\n",
    "for species in all_species\n",
    "    species_ = join(split(species), '_')\n",
    "#     @show species_\n",
    "    if occursin(species_ * \":\", newick)\n",
    "        inds = findall(isequal(species), raw_data.Species)\n",
    "        individuals = raw_data.Individual[inds]\n",
    "        newick_block = \"(\" * join([x * \":0.0\" for x in individuals], ',') * \")\"\n",
    "        newick = replace(newick, species_ * \":\" => newick_block * \":\")\n",
    "    else\n",
    "        @warn \"Species $species not in tree\"\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the tree and trait data all have the same taxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conformed_trimmed_data, conformed_newick = conform_tree_and_data(trimmed_data, newick)\n",
    "conformed_colinear_data, _ = conform_tree_and_data(full_data, newick)\n",
    "\n",
    "colinear_csv = \"data_12_06.csv\"\n",
    "noColinear_csv = \"data_noColinear_12_06.csv\"\n",
    "\n",
    "CSV.write(noColinear_csv, conformed_trimmed_data)\n",
    "CSV.write(colinear_csv, conformed_colinear_data);\n",
    "\n",
    "trimmed_nwk = \"newick_12_06.nwk\"\n",
    "\n",
    "write(trimmed_nwk, conformed_newick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Phylogenetic Factor Analysis (PFA)\n",
    "\n",
    "## 2.1 - Run PFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BEASTXMLConstructor.NewBEASTXMLConstructor\n",
    "using BEASTDataPrep\n",
    "using CSV\n",
    "using DataFrames\n",
    "using BeastUtils.RunBeast\n",
    "using LinearAlgebra\n",
    "\n",
    "day_df = raw_data[!, [\"Individual\", \"Species\", \"daytime\"]]\n",
    "\n",
    "\n",
    "species = unique(day_df.Species)\n",
    "n_species = length(species)\n",
    "is_day = zeros(Int, n_species)\n",
    "\n",
    "@assert sort(unique(day_df.daytime)) == [\"D\", \"N\"] # make sure there aren't any other codes I'm not looking for\n",
    "for i = 1:n_species\n",
    "    daytime = @view day_df.daytime[day_df.Species .== species[i]]\n",
    "#     @show species[i]\n",
    "#     @show daytime\n",
    "    day_count = count(isequal(\"D\"), daytime)\n",
    "    day_count == 0 ? is_day[i] = 0 : is_day[i] = 1\n",
    "end\n",
    "\n",
    "assignments = Dict(species[i] => is_day[i] for i = 1:n_species)\n",
    "\n",
    "assignments[\"Discodoris sp\"] = 0 # the only \"day\" species were found hiding under rocks\n",
    "\n",
    "day_df.species_day = [assignments[x] for x in day_df.Species]\n",
    "\n",
    "\n",
    "trimmed_nwk = \"newick_12_06.nwk\"\n",
    "activity_csv = \"activity.csv\"\n",
    "\n",
    "\n",
    "df = DataFrame(taxon = day_df.Individual, diel = day_df.species_day)\n",
    "df, _ = conform_tree_and_data(df, trimmed_nwk)\n",
    "CSV.write(activity_csv, df);   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function replace_space(s::String)\n",
    "    return join(split(s), '_')\n",
    "end\n",
    "\n",
    "function unreplace_space(s::String)\n",
    "    return join(split(s, '_'), ' ')\n",
    "end\n",
    "\n",
    "function merge_by_species(df::DataFrame, species_dict::Dict{String, String}; take_average::Bool = true)\n",
    "    taxa = df.taxon\n",
    "    data = Matrix(df[!, 2:end])\n",
    "    \n",
    "    species = unique(values(species_dict))\n",
    "    n_species = length(species)\n",
    "    \n",
    "    species_inds = [findall(x -> species_dict[x] == s, taxa) for s in species]\n",
    "    \n",
    "    found_species = findall(x -> length(x) > 0, species_inds)\n",
    "    \n",
    "    species = species[found_species]\n",
    "    species_inds = species_inds[found_species]\n",
    "    n_species = length(species)\n",
    "    \n",
    "    nms = names(df)\n",
    "    new_df = DataFrame(taxon = replace_space.(species))\n",
    "    for i = 2:length(nms)\n",
    "        new_df[!, nms[i]] = zeros(n_species)\n",
    "    end\n",
    "    \n",
    "    for i = 1:n_species\n",
    "        s_data = @view data[species_inds[i], :]\n",
    "        new_data = take_average ? vec(missing_mean(s_data)) : s_data[1, :]\n",
    "        new_df[i, 2:end] .= new_data\n",
    "    end\n",
    "    \n",
    "    return new_df\n",
    "end\n",
    "\n",
    "function merge_by_species(input::String, output::String, species_dict::Dict{String, String}; kwargs...)\n",
    "    df = CSV.read(input, DataFrame)\n",
    "    df2 = merge_by_species(df, species_dict; kwargs...)\n",
    "    CSV.write(output, df2)\n",
    "end\n",
    "\n",
    "function merge_by_species(input::String, species_dict::Dict{String, String}; kwargs...)\n",
    "    bn, ext = splitext(input)\n",
    "    output = bn * \"_bySpecies.\" * ext\n",
    "    merge_by_species(input, output, species_dict; kwargs...)\n",
    "    return output\n",
    "end\n",
    "    \n",
    "\n",
    "\n",
    "species_dict = Dict(raw_data.Individual[i] => raw_data.Species[i] for i = 1:size(raw_data, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function joint_xml(xml_path::String, color_csv::String, day_csv::String, newick_path::String; factors::Int = 2, chain_length::Int = 10000)\n",
    "    color_data = parse_traitdata(color_csv, trait_name = \"color\")\n",
    "    day_data = parse_traitdata(day_csv, trait_name = \"activity\", discrete_traits = [1])\n",
    "    \n",
    "    color_model = FactorModel(color_data, factors, standardize = true)\n",
    "    \n",
    "    p_data = size(day_data, 2)\n",
    "    day_model = RepeatedMeasuresModel(day_data, Diagonal(fill(10.0, p_data)), standardize = true)\n",
    "    \n",
    "    newick = read(newick_path, String)\n",
    "    \n",
    "    joint_model = JointTraitModel([color_model, day_model], newick)\n",
    "    options = MCMCOptions(chain_length = chain_length)\n",
    "    xml = save_xml(joint_model, xml_path, loadings_operator = \"gibbs\", mcmc_options = options, blombergs_k = true)\n",
    "end\n",
    "\n",
    "function file_name(bn::String, opt::String, k::Int)\n",
    "    return \"$(bn)_$(opt)_$(k)factors\"\n",
    "end\n",
    "    \n",
    "\n",
    "merge_species = false\n",
    "\n",
    "this_opt = 1:2\n",
    "\n",
    "bn = \"nudibranch_mean_k_all\"\n",
    "opts = [\"full\", \"sub\"][this_opt]\n",
    "csvs = [\"data_12_06.csv\", \"data_noColinear_12_06.csv\"][this_opt]\n",
    "activity_csv = \"activity.csv\"\n",
    "trimmed_nwk = \"newick_12_06.nwk\"\n",
    "\n",
    "\n",
    "if merge_species\n",
    "    global trimmed_nwk\n",
    "    csvs = [merge_by_species(c, species_dict) for c in csvs]\n",
    "    _,  new_newick = conform_tree_and_data(CSV.read(csvs[1], DataFrame), original_newick_path)\n",
    "    trimmed_nwk = splitext(trimmed_nwk)[1] * \"_bySpecies.nwk\"\n",
    "    write(trimmed_nwk, new_newick)\n",
    "\n",
    "    activity_csv = merge_by_species(activity_csv, species_dict, take_average = false)\n",
    "end\n",
    "n_opts = length(opts)\n",
    "ks = [1, 2, 3, 4, 5, 6];\n",
    "\n",
    "k_opts = [(k, opt) for k in ks, opt in 1:n_opts]\n",
    "fns = [file_name(bn, opt, k) for k in ks, opt in opts]\n",
    "xml_paths = fns .* \".xml\"\n",
    "log_paths = fns .* \".log\"\n",
    "rotated_paths = fns .* \"_processed.log\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# color_csv = \"data_noColinear_12_06.csv\"\n",
    "# color_csv = \"data_12_06.csv\"\n",
    "# bn = \"color_and_activity_full\"\n",
    "# xml_path = \"$bn.xml\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lk = ReentrantLock()\n",
    "Threads.@threads for (k, i) in k_opts\n",
    "    xml_path = xml_paths[k, i]\n",
    "\n",
    "    lock(lk) do \n",
    "        println(\"starting $xml_path ...\")\n",
    "    end\n",
    "\n",
    "    joint_xml(xml_path, csvs[i], activity_csv, trimmed_nwk, factors = k, chain_length = 10_000)\n",
    "    out_path = splitext(xml_path)[1] * \".out\"\n",
    "    if !(isfile(out_path) && occursin(\"Operator analysis\", read(out_path, String)))\n",
    "        run_beast(xml_path, capture_output = true, beast_jar = \"beast.jar\")\n",
    "    end\n",
    "\n",
    "    println(\"... finished $xml_path\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PhylogeneticFactorAnalysis.BEASTPostProcessing\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "\n",
    "\n",
    "function data_stats(path::String)\n",
    "    df = CSV.read(path, DataFrame)\n",
    "    n, p = size(df)\n",
    "    p -= 1\n",
    "    return n, p\n",
    "end\n",
    "\n",
    "n, p_activity = data_stats(activity_csv)\n",
    "\n",
    "for i in 1:n_opts\n",
    "    n, p_color = data_stats(csvs[i])\n",
    "\n",
    "    Threads.@threads for k in ks\n",
    "        fn = file_name(bn, opts[i], k)\n",
    "        log_path = log_paths[k, i]\n",
    "        rotated_path = rotated_paths[k, i]\n",
    "\n",
    "        post_process(log_path, rotated_path, \n",
    "                     [\"color\" => (k, p_color), \"activity\" => (p_activity, p_activity)],\n",
    "                     n,\n",
    "                     optimization_inds=[[k + 1], Int[]])\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Determine the number of factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RCall\n",
    "using BeastUtils.Logs\n",
    "\n",
    "\n",
    "function collect_first_loadings(log_path::String)\n",
    "    df = import_log(log_path, burnin = 0.1)\n",
    "    load_df = df[!, findall(x -> startswith(x, \"color.L.1\"), names(df))]\n",
    "    L1 = vec(mean(Matrix(load_df), dims=1))\n",
    "    return L1\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "pair_dfs = [DataFrame() for _ = 1:n_opts]\n",
    "\n",
    "for (k, i) in k_opts\n",
    "    rotated_path = rotated_paths[k, i]\n",
    "    L1 = collect_first_loadings(rotated_path)\n",
    "    pair_dfs[i][!, \"k=$k\"] = L1\n",
    "end\n",
    "\n",
    "\n",
    "svg_paths = [\"pairs_$(opt).svg\" for opt in opts]\n",
    "@rput pair_dfs\n",
    "@rput svg_paths\n",
    "\n",
    "R\"\"\"\n",
    "\n",
    "for (i in 1:length(pair_dfs)) {\n",
    "    svg(svg_paths[i])\n",
    "    pairs(pair_dfs[i])\n",
    "    dev.off()\n",
    "}\n",
    "\"\"\"\n",
    "for i = 1:n_opts\n",
    "    println(opts[i])\n",
    "    open(svg_paths[i]) do f\n",
    "       display(\"image/svg+xml\", read(f, String))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the plots above, the loadings associated with the first factor stabilize after k = 4 factors.\n",
    "As such, we settle on a model with 4 factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Check Model Equivalence\n",
    "\n",
    "We double check that the full model with all traits and the smaller model where colinear traits have been removed indeed return equivalent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gadfly\n",
    "\n",
    "\n",
    "labels = Vector{String}[]\n",
    "\n",
    "for path in csvs\n",
    "    l = names(CSV.read(path, DataFrame))[2:end]\n",
    "    push!(labels, l)\n",
    "end\n",
    "\n",
    "in_all = intersect(labels...)\n",
    "inds = [[findfirst(x -> x == y, labs) for y in in_all] for labs in labels]\n",
    "\n",
    "paired = [DataFrame() for _ in ks]\n",
    "\n",
    "for k in ks\n",
    "    for i = 1:n_opts\n",
    "        rotated_path = rotated_paths[k, i]\n",
    "        li = collect_first_loadings(rotated_path)[inds[i]]\n",
    "        df = paired[k]\n",
    "        df[!, opts[i]] = li\n",
    "    end\n",
    "end\n",
    "        \n",
    "ps = [plot(paired[k], x = :sub, y = :full, Geom.point, Guide.title(\"k = $k\")) for k in ks]\n",
    "p = vstack(ps)\n",
    "img_path = \"loadings_correlations.svg\"\n",
    "img = SVG(img_path, 4inch, 20inch)\n",
    "draw(img, p)\n",
    "open(img_path) do f\n",
    "   display(\"image/svg+xml\", read(f, String))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the plots above, the loadings associated with the first factor are essentially the same between the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BeastUtils.Logs\n",
    "using PhylogeneticFactorAnalysis\n",
    "using RCall\n",
    "\n",
    "function extract_species(s::AbstractString)\n",
    "    return split(s, '_')[2]\n",
    "end\n",
    "\n",
    "function clean_taxa(taxa::AbstractArray{<:AbstractString}, dict::Dict{String, String})\n",
    "    species = [dict[taxon] for taxon in taxa]\n",
    "    u_species = unique(species)\n",
    "    firsts = [findfirst(isequal(s), species) for s in u_species]\n",
    "    keep_taxa = taxa[firsts]\n",
    "    df = DataFrame(original = keep_taxa, new = species[firsts])\n",
    "    return df\n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_paths = fns .* \"_correlation.pdf\"\n",
    "\n",
    "cd(@__DIR__)\n",
    "\n",
    "\n",
    "new_species_dict = Dict(raw_data.Individual[i] => raw_data.Species[i] for i in 1:size(raw_data, 1))\n",
    "if merge_species\n",
    "    new_species_dict = Dict(replace_space(raw_data.Species[i]) => raw_data.Species[i] for i in 1:size(raw_data, 1))\n",
    "end\n",
    "\n",
    "\n",
    "trait_names = [\"color\", \"activity\"]\n",
    "for i = 1:n_opts\n",
    "    color = CSV.read(csvs[i], DataFrame)\n",
    "    trait_labels = names(color)[2:end]\n",
    "    all_taxa = color.taxon\n",
    "    new_taxa = clean_taxa(all_taxa, new_species_dict)\n",
    "    for k in ks\n",
    "        rotated_path = rotated_paths[k, i]\n",
    "        correlation_path = correlation_paths[k, i]\n",
    "\n",
    "        bl = LazyLog(rotated_path)\n",
    "\n",
    "        plot_correlation(bl, correlation_path, [[\"color.$i\" for i = 1:k]; \"activity\"])\n",
    "        plot_loadings(bl, trait_names, [k, p_activity], file_base = fns[k, i], factor_partitions = [1], original_labels = [trait_labels])\n",
    "\n",
    "        PhylogeneticFactorAnalysis.prep_factors(bl, \"tmp_factors.csv\",\n",
    "                fac_header = \"color.activity.joint.\",\n",
    "                k = sum(k + p_activity))\n",
    "\n",
    "\n",
    "\n",
    "        PhylogeneticFactorAnalysis.prep_r_factors(\n",
    "            \"color\",\n",
    "            \"tmp_factors.csv\",\n",
    "            trimmed_nwk,\n",
    "            \"\",\n",
    "            layout = \"rectangular\",\n",
    "            fac_names = [\"color.$i\" for i = 1:k],\n",
    "            tip_labels = true,\n",
    "            line_width = 1,\n",
    "            include_only = new_taxa.original,\n",
    "            relabel = new_taxa\n",
    "        )\n",
    "\n",
    "        factors = collect(1:k)\n",
    "        @rput factors\n",
    "        plot_path = fns[k, i]\n",
    "        @rput plot_path\n",
    "\n",
    "        R\"\"\"\n",
    "        source(R_PLOT_SCRIPT)\n",
    "\n",
    "        fac_names <- optional_arguments[[1]]\n",
    "        include_only <- optional_arguments[[2]]\n",
    "        relabel <- optional_arguments[[3]]\n",
    "\n",
    "\n",
    "        plot_factor_tree(plot_path, tree_path, stats_path, class_path=class_array[[1]],\n",
    "                         fac_names = fac_names, layout = layout,\n",
    "                         tip_labels = tip_labels, line_width = line_width,\n",
    "                         include_only = include_only, relabel = relabel,\n",
    "                         extra_offset = 0.2,\n",
    "                         labels_offset = 0.02,\n",
    "                         width=12,\n",
    "                         factors = factors)\n",
    "        \"\"\"\n",
    "    end\n",
    "end\n",
    "\n",
    "rm(\"tmp_factors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = \"sub\"\n",
    "opti = findfirst(x -> x == opt, opts)\n",
    "color_csv = csvs[opti]\n",
    "# trimmed_nwk = \"newick_12_06.nwk\"\n",
    "# activity_csv = \"activity.csv\"\n",
    "\n",
    "labels_csv = \"color_labels.csv\"\n",
    "color_names = names(CSV.read(color_csv, DataFrame))[2:end]\n",
    "\n",
    "k = 4\n",
    "processed_log = rotated_paths[k, opti]\n",
    "tmp_csv = \"tmp.csv\"\n",
    "PhylogeneticFactorAnalysis.prep_loadings(processed_log, \n",
    "    tmp_csv, \n",
    "    k = k, n_traits = k + 1, L_header = \"color.L.\", fac_header = \"color.activity.joint\",\n",
    "    original_labels = color_names)\n",
    "\n",
    "cat_dict = Dict(\"Col\" => \"LEIA\",\n",
    "                \"Lum\" => \"LEIA\",\n",
    "                \"GabRat\" => \"\",\n",
    "                \"CAA\" => \"CAA\",\n",
    "                \"VCA\" => \"VCA\",\n",
    "                \"BSA\" => \"BSA\")\n",
    "\n",
    "type_dict = Dict(\"mean\" => \"mean\",\n",
    "                 \"sd\" => \"sd\",\n",
    "                 \"CoV\" => \"CoV\",\n",
    "                 \"skew\" => \"skew\",\n",
    "                 \"kurtosis\" => \"kurtosis\",\n",
    "                 \"Sc\" => \"Sc\",\n",
    "                 \"Jc\" => \"Jc\",\n",
    "                 \"St\" => \"St\",\n",
    "                 \"Jt\" => \"Jt\",\n",
    "                 \"Hc\" => \"Hc\",\n",
    "                 \"Qc\" => \"Qc\",\n",
    "                 \"Qt\" => \"Qt\",\n",
    "                 \"Ht\" => \"Ht\",\n",
    "                 \"Scpl\" => \"Scpl\",\n",
    "                 \"Qcpl\" => \"Qcpl\",\n",
    "                 \"C\" => \"C\",\n",
    "                 \"PT\" => \"PT\",\n",
    "                 \"Asp\" => \"Asp\",\n",
    "                 \"ML\" => \"ML\",\n",
    "                 \"sL\" => \"sL\",\n",
    "                 \"CVL\" => \"CVL\",\n",
    "                 \"MDmax\" => \"MDmax\",\n",
    "                 \"sDmax\" => \"sDmax\",\n",
    "                 \"CVDmax\" => \"CVDmax\",\n",
    "                 \"MSsat\" => \"MSsat\",\n",
    "                 \"sSsat\" => \"sSsat\",\n",
    "                 \"CVSsat\" => \"CVSsat\",\n",
    "                 \"MSL\" => \"MSL\",\n",
    "                 \"sSL\" => \"sSL\",\n",
    "                 \"CVSL\" => \"CVSL\",\n",
    "                 \"MS\" => \"MS\",\n",
    "                 \"sS\" => \"sS\",\n",
    "                 \"CVS\" => \"CVS\",\n",
    "                 \"BML\" => \"BML\",\n",
    "                 \"BsL\" => \"BsL\",\n",
    "                 \"BCVL\" => \"BCVL\",\n",
    "                 \"BMDmax\" => \"BMDmax\",\n",
    "                 \"BsDmax\" => \"BsDmax\",\n",
    "                 \"BCVDmax\" => \"BCVDmax\",\n",
    "                 \"BMSsat\" => \"BMSsat\",\n",
    "                 \"BsSsat\" => \"BsSsat\",\n",
    "                 \"BCVSsat\" => \"BCVSsat\",\n",
    "                 \"BMSL\" => \"BMSL\",\n",
    "                 \"BsSL\" => \"BsSL\",\n",
    "                 \"BCVSL\" => \"BCVSL\",\n",
    "                 \"BMS\" => \"BMS\",\n",
    "                 \"BsS\" => \"BsS\",\n",
    "                 \"BCVS\" => \"BCVS\")\n",
    "subtype_dict = Dict(\"Vrt\" => \"vertical\",\n",
    "                    \"vrt\" => \"vertical\",\n",
    "                    \"hrz\" => \"horizontal\",\n",
    "                    \"Hrz\" => \"horizontal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = length(color_names)\n",
    "split_names = [split(x, '.') for x in color_names]\n",
    "cats = fill(\"\", p)\n",
    "types = deepcopy(cats)\n",
    "\n",
    "for i = 1:p\n",
    "    sn = split(color_names[i], '.')\n",
    "    cats[i] = cat_dict[sn[1]]\n",
    "    if length(sn) == 1\n",
    "        types[i] = sn[1]\n",
    "    elseif length(sn) == 2\n",
    "        if cats[i] == \"LEIA\"\n",
    "            types[i] *= sn[1] * \".\"\n",
    "        end\n",
    "        types[i] *= type_dict[sn[2]]\n",
    "    elseif length(sn) == 3\n",
    "        types[i] *= type_dict[sn[2]] * \" (\" * subtype_dict[sn[3]] * \")\"\n",
    "    elseif length(sn) == 1\n",
    "#         types[i] = cats[i]\n",
    "    else        \n",
    "        error(\"not implemented\")\n",
    "    end\n",
    "end\n",
    "\n",
    "df = DataFrame(trait = color_names, pretty = types, cat = cats)\n",
    "CSV.write(labels_csv, df)\n",
    "\n",
    "r_script = PhylogeneticFactorAnalysis.R_PLOT_SCRIPT\n",
    "@rput r_script\n",
    "@rput tmp_csv\n",
    "@rput labels_csv\n",
    "R\"\"\"\n",
    "source(r_script)\n",
    "plot_loadings(tmp_csv, \"nudibranch_loadings.pdf\", labels_path = labels_csv, factors=c(1), width_scale=1.5)\n",
    "\"\"\"\n",
    "rm(tmp_csv)\n",
    "# rm(labels_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PhylogeneticFactorAnalysis\n",
    "using RCall\n",
    "using CSV\n",
    "\n",
    "color = CSV.read(csvs[opti], DataFrame)\n",
    "trait_labels = names(color)[2:end]\n",
    "all_taxa = color.taxon\n",
    "new_taxa = clean_taxa(all_taxa, new_species_dict)\n",
    "CSV.write(\"taxon_dict.csv\", new_taxa)\n",
    "tmp_csv = \"tmp_factors.csv\"\n",
    "activity_df = CSV.read(activity_csv, DataFrame)\n",
    "activity_dict = Dict(0 => \"night only\", 1 => \"day\")\n",
    "labels_df = DataFrame(taxon = activity_df.taxon, activity = [activity_dict[x] for x in activity_df.diel])\n",
    "labels_csv = \"activity_labels.csv\"\n",
    "CSV.write(labels_csv, labels_df)\n",
    "\n",
    "PhylogeneticFactorAnalysis.prep_factors(rotated_paths[k, opti], tmp_csv,\n",
    "        fac_header = \"color.activity.joint.\",\n",
    "        k = sum(k + p_activity))\n",
    "\n",
    "PhylogeneticFactorAnalysis.prep_r_factors(\n",
    "    \"color\",\n",
    "    \"tmp_factors.csv\",\n",
    "    trimmed_nwk,\n",
    "    labels_csv,\n",
    "    layout = \"rectangular\",\n",
    "    fac_names = [\"color.$i\" for i = 1:k],\n",
    "    tip_labels = true,\n",
    "    line_width = 1,\n",
    "    include_only = new_taxa.original,\n",
    "    relabel = new_taxa\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = collect(1:k)\n",
    "@rput factors\n",
    "plot_path = fns[k, opti] * \"_final\"\n",
    "@rput plot_path\n",
    "\n",
    "R\"\"\"\n",
    "source(R_PLOT_SCRIPT)\n",
    "\n",
    "fac_names <- optional_arguments[[1]]\n",
    "include_only <- optional_arguments[[2]]\n",
    "relabel <- optional_arguments[[3]]\n",
    "\n",
    "print(tree_path)\n",
    "print(relabel)\n",
    "\n",
    "\n",
    "plot_factor_tree(plot_path, tree_path, stats_path, class_path=class_array[[1]],\n",
    "                 fac_names = c(\"factor 1\"), layout = layout,\n",
    "                 tip_labels = tip_labels, line_width = line_width,\n",
    "                 include_only = include_only, relabel = relabel,\n",
    "                 extra_offset = 1,\n",
    "                 labels_offset = 0.02,\n",
    "                 width=7,\n",
    "                 factors = c(1),\n",
    "                 combined=FALSE)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_activity_csv = merge_by_species(activity_csv, species_dict, take_average = false)\n",
    "species_factor_csv = merge_by_species(\"tmp_factors.csv\", species_dict, take_average = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Predictive Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GLM\n",
    "\n",
    "facs = CSV.read(species_factor_csv, DataFrame)\n",
    "acs = CSV.read(species_activity_csv, DataFrame)\n",
    "\n",
    "logit_df = innerjoin(facs, acs, on = :taxon)\n",
    "fit = glm(@formula(diel ~ 1 + f1), logit_df, Bernoulli(), LogitLink())\n",
    "ps = predict(fit)\n",
    "thresholds = 0.5:0.01:1.0\n",
    "n = length(thresholds)\n",
    "probs = zeros(n)\n",
    "for i = 1:n\n",
    "    threshold = thresholds[i]\n",
    "    diel_p = ps .> threshold\n",
    "    m = nrow(logit_df)\n",
    "    probs[i] = sum(diel_p .== logit_df.diel) / m\n",
    "end\n",
    "\n",
    "@show findmax(probs)\n",
    "best_threshold = thresholds[findmax(probs)[2]]\n",
    "best_assigned = ps .> best_threshold\n",
    "\n",
    "\n",
    "plot(x = predict(fit), y = logit_df.diel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sortperm(logit_df.f1)\n",
    "logit_df = logit_df[sp, :]\n",
    "n = nrow(logit_df)\n",
    "Ms = zeros(Int, n, 2, 2)\n",
    "tc_rates = zeros(n)\n",
    "for i = 1:n\n",
    "    sb = sum(logit_df.diel[1:i])\n",
    "    sa = sum(logit_df.diel[(i + 1):n])\n",
    "    Ms[i, 1, 1] = i - sb\n",
    "    Ms[i, 1, 2] = sb\n",
    "    Ms[i, 2, 1] = n - i - sa\n",
    "    Ms[i, 2, 2] = sa\n",
    "    tc_rates[i] = (sa + i - sb) / n\n",
    "end\n",
    "\n",
    "best_rate = maximum(tc_rates)\n",
    "max_inds = findall(isequal(best_rate), tc_rates)\n",
    "ind = max_inds[1]\n",
    "Ms[ind, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above indicate that we can accurately predict activity patterns in 89% of species using only factor 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5 * (logit_df.f1[ind] + logit_df.f1[ind + 1])\n",
    "logit_df.class = logit_df.f1 .< threshold\n",
    "@rput threshold\n",
    "\n",
    "logit_df.pretty_diel = [Dict(1 => \"day\", 0 => \"night only\")[i] for i in logit_df.diel]\n",
    "\n",
    "logit_df.class_name = [x ? \"night only\" : \"day\" for x in logit_df.class]\n",
    "logit_df.correct_prediction = [x == y for (x, y) in zip(logit_df.class_name, logit_df.pretty_diel)]\n",
    "print(logit_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@rput logit_df\n",
    "\n",
    "m = logit_df.f1[1] - 1e-4\n",
    "x = logit_df.f1[end] + 1e-4\n",
    "n = 50\n",
    "thresholds = range(x, m, length = n)\n",
    "f1 = logit_df.f1\n",
    "diel = logit_df.diel\n",
    "class_pos = [sum(f1 .> t) for t in thresholds]\n",
    "true_pos = [sum((f1 .> t) .&& (diel .== 1) ) for t in thresholds]\n",
    "false_pos = class_pos .- true_pos\n",
    "tpr = true_pos ./ sum(diel)\n",
    "fpr = false_pos ./ (nrow(logit_df) - sum(diel))\n",
    "\n",
    "roc_df = DataFrame(fpr = fpr, tpr = tpr)\n",
    "\n",
    "aurocs = [0.5 * (roc_df.tpr[i] + roc_df.tpr[i - 1]) * (roc_df.fpr[i] - roc_df.fpr[i - 1]) for i = 2:nrow(roc_df)]\n",
    "auroc = sum(aurocs)\n",
    "@show auroc\n",
    "\n",
    "@rput auroc\n",
    "@rput roc_df\n",
    "R\"\"\"\n",
    "library(ggplot2)\n",
    "ggplot(logit_df, aes(y = f1, x = pretty_diel, color = class, shape = correct_prediction)) +\n",
    "    geom_point() +\n",
    "    geom_hline(yintercept = threshold) +\n",
    "    xlab(\"True Activity Pattern\") + \n",
    "    ylab(\"factor 1\") +\n",
    "    theme_bw() +\n",
    "    scale_color_discrete(labels = c(\"TRUE\" = \"night only\", \"FALSE\" = \"day\"), \n",
    "                         name = \"Predicted Activity\\nPattern\") +\n",
    "    scale_shape_manual(labels = c(\"TRUE\" = \"correct\", \"FALSE\" = \"incorrect\"),\n",
    "                         name = \"Prediction Accuracy\",\n",
    "                         values = c(4, 19))\n",
    "ggsave(\"separation.svg\", height = 4, width = 4)\n",
    "\n",
    "ggplot(roc_df, aes(x = fpr, y = tpr)) +\n",
    "    geom_path() +\n",
    "    geom_segment(x = 0, y = 0, xend = 1, yend = 1, linetype=\"dashed\") +\n",
    "    xlab(\"false positive rate\") +\n",
    "    ylab(\"true positive rate\") +\n",
    "    annotate(\"text\", x = 0.75, y = 0.325, label = paste0(\"AUROC = \", round(auroc, digits = 2))) +\n",
    "    theme_bw()\n",
    "\n",
    "ggsave(\"roc.svg\", height=4, width = 4)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"roc.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve an area under the ROC curve (AUROC) of 0.94.\n",
    "For reference, a completely random classifier has ROC = 0.5, and a perfect classifier has ROC = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HypothesisTests\n",
    "\n",
    "f_day = logit_df.f1[logit_df.diel .== 1]\n",
    "f_night = logit_df.f1[logit_df.diel .== 0]\n",
    "\n",
    "test = UnequalVarianceTTest(f_day, f_night)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resutls above indicate a highly significant relationship between factor 1 and diel activity (p < 1e-5)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
